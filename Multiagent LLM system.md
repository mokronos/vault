
## Random Thoughs

- Run locally with small fast model to test
	- tinyllama is really fast
		- good for sanity checking bugs
		- but doesn't really follow prompts well
	- with decent gpu llama3 is probably best
		- still decently fast
		- follows prompts well
	- not sure what prompt/performance difference better models make, but i assume that the better the model, the better the whole system; it will probably be necessary to balance performance and cost at some point for specific agents